import json  
from pymongo import MongoClient  
from datetime import datetime  
import logging  
from pyapacheatlas.auth import BasicAuthentication  
from pyapacheatlas.core import AtlasClient  
import requests  
import time
  
ATLAS_URL = "http://172.17.0.1:21000"  
ATLAS_USER = "admin"  
ATLAS_PASS = "ensias123@"  
MONGO_URI = 'mongodb://mongodb:27017/'  
  
logging.basicConfig(level=logging.INFO)  
logger = logging.getLogger(__name__)  
  
class AtlasMetadataGovernance:  
    def __init__(self):  
        self.atlas_url = ATLAS_URL  
        self.auth = BasicAuthentication(username=ATLAS_USER, password=ATLAS_PASS)  
        self.atlas_client = AtlasClient(  
            endpoint_url=self.atlas_url,  
            authentication=self.auth  
        )  
        self.auth_tuple = (ATLAS_USER, ATLAS_PASS)  
          
        self.mongo_client = MongoClient(MONGO_URI)  
        self.metadata_db = self.mongo_client['metadata_validation_db']  
        self.current_glossary_guid = None  
        
        # Cache pour √©viter de rechercher plusieurs fois les m√™mes termes
        self.created_terms_cache = {}
        # Cache pour les colonnes Hive r√©elles (cl√© de votre mapping)
        self.hive_columns_cache = {}
          
        self._test_connections()  
  
    def _test_connections(self):  
        """Tester les connexions Atlas et MongoDB"""  
        logger.info("üîç Test des connexions...")  
          
        try:  
            self.mongo_client.admin.command('ping')  
            logger.info("‚úÖ MongoDB connect√©")  
        except Exception as e:  
            logger.error(f"‚ùå MongoDB non accessible: {e}")  
            raise  
  
    # ==================== PARTIE 1: D√âCOUVERTE DES COLONNES HIVE ====================
    # Cette partie correspond au "scan" dans Purview: on d√©couvre ce qui existe r√©ellement
    
    def discover_hive_schema(self, table_name):
        """
        √âTAPE 1 (comme Purview): Scanner la table Hive pour d√©couvrir son sch√©ma r√©el.
        
        Cette fonction remplace votre logique de "mapping intelligent" par une d√©couverte 
        bas√©e sur ce qui existe r√©ellement dans Hive, comme le fait Purview.
        
        Returns:
            dict: Mapping {nom_colonne_hive_lowercase: info_colonne}
        """
        logger.info(f"üîç √âTAPE 1: D√©couverte du sch√©ma Hive pour table '{table_name}'")
        
        try:
            # 1. Trouver la table dans Atlas
            table_guid = self.get_hive_table_entity(table_name)
            if not table_guid:
                logger.error(f"‚ùå Table {table_name} non trouv√©e")
                return {}
            
            # 2. R√©cup√©rer les colonnes r√©elles
            columns = self.get_table_columns(table_guid)
            
            # 3. Cr√©er un index des colonnes (normalis√© en lowercase pour matching)
            hive_schema = {}
            for col in columns:
                # Normaliser le nom pour le matching (lowercase, sans espaces)
                normalized_name = col['name'].lower().strip().replace(' ', '_')
                hive_schema[normalized_name] = {
                    'guid': col['guid'],
                    'original_name': col['name'],  # Nom original Hive
                    'type': col['type']
                }
                logger.info(f"  üìã Colonne Hive d√©couverte: {col['name']} ‚Üí {normalized_name}")
            
            # Sauvegarder dans le cache
            self.hive_columns_cache[table_name] = hive_schema
            
            logger.info(f"‚úÖ {len(hive_schema)} colonnes Hive d√©couvertes")
            return hive_schema
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©couverte sch√©ma: {e}")
            return {}
    
    def get_hive_table_entity(self, table_name):  
        """R√©cup√©rer l'entit√© table Hive via API REST"""  
        try:  
            search_url = f"{self.atlas_url}/api/atlas/v2/search/dsl"  
            response = requests.get(  
                search_url,  
                auth=self.auth_tuple,  
                params={'query': f"hive_table where name='{table_name}'"}  
            )  
              
            if response.status_code == 200:  
                entities = response.json().get('entities', [])  
                if entities:  
                    return entities[0]['guid']  
        except Exception as e:  
            logger.error(f"Erreur recherche table: {e}")  
          
        return None  
  
    def get_table_columns(self, table_guid):  
        """R√©cup√©rer les colonnes via l'API REST"""  
        try:  
            entity_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{table_guid}"  
            response = requests.get(  
                entity_url,  
                auth=self.auth_tuple,  
                params={'ignoreRelationships': 'false'}  
            )  
              
            if response.status_code == 200:  
                entity = response.json()['entity']  
                columns = entity.get('relationshipAttributes', {}).get('columns', [])  
                  
                column_info = []  
                for col in columns:  
                    column_info.append({  
                        'guid': col['guid'],  
                        'name': col['displayText'],  
                        'type': col['typeName']  
                    })  
                return column_info  
        except Exception as e:  
            logger.error(f"Erreur r√©cup√©ration colonnes: {e}")  
          
        return []  

    # ==================== PARTIE 2: CR√âATION DU GLOSSAIRE ET TAXONOMIE ====================
    # Cette partie cr√©e la structure business (glossaire + cat√©gories + termes)
    
    def create_business_glossary(self):  
        """
        √âTAPE 2: Cr√©er le glossaire business.
        
        Dans Purview, le glossaire est cr√©√© manuellement ou via API.
        Ici on le cr√©e automatiquement avec une structure RGPD.
        """  
        try:  
            from AtlasAPI.atlas_integration import CustomAtlasGlossary  
              
            glossary = CustomAtlasGlossary(  
                name="Data_Governance_Glossary_PyAtlas5",  
                shortDescription="Glossaire m√©tier pour la gouvernance des donn√©es",  
                longDescription="Glossaire centralis√© utilisant pyapacheatlas"  
            )  
              
            response = requests.post(  
                f"{self.atlas_url}/api/atlas/v2/glossary",  
                json=glossary.to_dict(),  
                auth=self.auth_tuple,  
                timeout=(30, 60)  
            )  
              
            if response.status_code == 200:  
                glossary_guid = response.json()['guid']  
                self.current_glossary_guid = glossary_guid  
                logger.info(f"‚úÖ Glossaire cr√©√©: {glossary_guid}")  
                return glossary_guid  
            else:  
                logger.error(f"‚ùå √âchec cr√©ation glossaire: {response.text}")  
                return None  
                  
        except Exception as e:  
            logger.error(f"‚ùå Erreur cr√©ation glossaire: {e}")  
            return None  
    
    def create_rgpd_categories(self, glossary_guid):  
        """
        √âTAPE 3: Cr√©er les cat√©gories RGPD (taxonomie business).
        
        CORRECTION IMPORTANTE: Les cat√©gories doivent √™tre cr√©√©es AVANT les termes
        pour pouvoir les r√©f√©rencer lors de la cr√©ation des termes.
        """  
        real_categories = self.extract_rgpd_categories_from_db()  
        category_guids = {}  
          
        category_descriptions = {  
            "Donn√©es d'identification": "Informations permettant d'identifier directement ou indirectement une personne physique",  
            "Donn√©es financi√®res": "Informations bancaires, financi√®res et de paiement",  
            "Donn√©es de contact": "Informations de contact et de communication",  
            "Donn√©es de localisation": "Informations g√©ographiques et d'adresse",  
            "Donn√©es temporelles": "Informations de date, heure et temporelles",  
            "Donn√©es de sant√©": "Informations m√©dicales et de sant√©",  
            "Donn√©es biom√©triques": "Donn√©es biom√©triques d'identification",  
            "Donn√©es de comportement": "Donn√©es de navigation et comportementales"  
        }  
          
        for category in real_categories:  
            cat_data = {  
                "name": category,  
                "shortDescription": category_descriptions.get(category, f"Cat√©gorie RGPD: {category}"),  
                "longDescription": f"Cat√©gorie de donn√©es personnelles selon le RGPD: {category}",  
                "anchor": {"glossaryGuid": glossary_guid}  
            }  
              
            response = requests.post(  
                f"{self.atlas_url}/api/atlas/v2/glossary/category",  
                json=cat_data,  
                auth=self.auth_tuple  
            )  
              
            if response.status_code == 200:  
                category_guids[category] = response.json()['guid']  
                logger.info(f"‚úÖ Cat√©gorie cr√©√©e: {category} ‚Üí {category_guids[category]}")  
            else:  
                logger.error(f"‚ùå Erreur cat√©gorie {category}: {response.text}")  
          
        return category_guids  
    
    def extract_rgpd_categories_from_db(self):  
        """Extraire les cat√©gories RGPD depuis MongoDB"""  
        enriched_metadata = self.metadata_db['enriched_metadata']  
        categories = enriched_metadata.distinct('recommended_rgpd_category')  
        categories = [cat for cat in categories if cat and cat.strip()]  
        logger.info(f"Cat√©gories RGPD trouv√©es: {categories}")  
        return categories  

    def create_validated_metadata_terms(self, glossary_guid, category_guids, hive_columns_map):
     """
     CORRECTION FINALE: Cr√©er les termes PUIS assigner aux colonnes Hive
    
     L'API Atlas v2 ne permet PAS d'inclure assignedEntities √† la cr√©ation.
     Il faut faire en 2 √©tapes s√©par√©es.
     """
     enriched_metadata = self.metadata_db['enriched_metadata']
     validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))
    
     synced_terms = 0
     terms_to_assign = []  # Pour assignation en phase 2
    
     for metadata in validated_metadata:
        column_name = metadata['column_name']
        job_id = metadata['job_id']
        rgpd_category = metadata.get('recommended_rgpd_category')
        
        # R√©cup√©rer le GUID Hive correspondant
        hive_column_guid = hive_columns_map.get(column_name.lower())
        
        term_name = f"{column_name.upper()}_TERM"
        qualified_name = f"datagovernance.{column_name}_{job_id}@production"
        
        try:
            from AtlasAPI.atlas_integration import CustomAtlasGlossaryTerm
            
            term = CustomAtlasGlossaryTerm(
                name=term_name,
                qualifiedName=qualified_name,
                shortDescription=f"Attribut m√©tier valid√©: {column_name}",
                longDescription=self._generate_business_description(metadata),
                attributes={
                    "source_column": column_name,
                    "source_dataset": job_id,
                    "entity_types": metadata.get('entity_types', []),
                    "sensitivity_level": metadata.get('recommended_sensitivity_level'),
                    "rgpd_category": rgpd_category
                }
            )
            
            # Ajouter classification
            sensitivity_level = metadata.get('recommended_sensitivity_level')
            if sensitivity_level:
                term.addClassification(
                    f"DataSensitivity_{sensitivity_level}",
                    {
                        "sensitivity_level": sensitivity_level,
                        "rgpd_compliant": True,
                        "data_steward": "Validated"
                    }
                )
            
            term.glossaryGuid = glossary_guid
            
            # Ajouter cat√©gorie RGPD
            if rgpd_category and rgpd_category in category_guids:
                term.categories = [{
                    "categoryGuid": category_guids[rgpd_category]
                }]
            
            # IMPORTANT: NE PAS inclure assignedEntities ici
            term_payload = term.to_dict()
            
            # Cr√©er le terme (SANS relation Hive)
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/glossary/term",
                json=term_payload,
                auth=self.auth_tuple
            )
            
            if response.status_code == 200:
                term_guid = response.json()['guid']
                
                # Sauvegarder dans le cache
                self.created_terms_cache[term_name] = {
                    'guid': term_guid,
                    'qualified_name': qualified_name,
                    'source_column': column_name,
                    'hive_column_guid': hive_column_guid  # Pour assignation ult√©rieure
                }
                
                # Pr√©parer pour assignation si colonne Hive existe
                if hive_column_guid:
                    terms_to_assign.append({
                        'term_guid': term_guid,
                        'term_name': term_name,
                        'column_guid': hive_column_guid,
                        'column_name': column_name
                    })
                
                logger.info(f"‚úÖ Terme cr√©√©: {term_name}")
                synced_terms += 1
            else:
                logger.error(f"‚ùå Erreur terme {term_name}: {response.text}")
                
        except Exception as e:
            logger.error(f"‚ùå Exception terme {term_name}: {e}")
    
     return synced_terms, terms_to_assign

    # ==================== PARTIE 3: MAPPING CSV ‚Üí HIVE ====================
    # Cette partie cr√©e le lien entre vos m√©tadonn√©es CSV et le sch√©ma Hive r√©el
    
    def create_csv_to_hive_mapping(self, table_name):
        """
        √âTAPE 5: Cr√©er le mapping entre colonnes CSV (MongoDB) et colonnes Hive r√©elles.
        
        CORRECTION IMPORTANTE: 
        - On se base sur le sch√©ma Hive R√âEL d√©couvert pr√©c√©demment
        - On normalise les noms pour matcher (lowercase, underscores)
        - On utilise une logique de "fuzzy matching" simple
        
        Cette approche est plus proche de Purview qui fait du matching automatique
        entre les sources d√©couvertes et les m√©tadonn√©es business.
        
        Returns:
            dict: {nom_colonne_hive: {term_info, column_info}}
        """
        logger.info(f"üîó √âTAPE 5: Cr√©ation du mapping CSV ‚Üí Hive")
        
        # 1. R√©cup√©rer le sch√©ma Hive si pas d√©j√† en cache
        if table_name not in self.hive_columns_cache:
            self.discover_hive_schema(table_name)
        
        hive_schema = self.hive_columns_cache.get(table_name, {})
        if not hive_schema:
            logger.error("‚ùå Aucun sch√©ma Hive disponible")
            return {}
        
        # 2. Pour chaque terme cr√©√©, essayer de trouver la colonne Hive correspondante
        mapping = {}
        matched_count = 0
        unmatched_csv = []
        
        for term_name, term_info in self.created_terms_cache.items():
            csv_column = term_info['source_column']
            
            # Normaliser le nom CSV pour matching
            normalized_csv = csv_column.lower().strip().replace(' ', '_')
            
            # Chercher la colonne Hive correspondante
            if normalized_csv in hive_schema:
                # MATCH EXACT trouv√© !
                hive_col_info = hive_schema[normalized_csv]
                mapping[hive_col_info['original_name']] = {
                    'term_guid': term_info['guid'],
                    'term_name': term_name,
                    'column_guid': hive_col_info['guid'],
                    'csv_column': csv_column,
                    'match_type': 'exact'
                }
                matched_count += 1
                logger.info(f"‚úÖ MATCH: CSV '{csv_column}' ‚Üí Hive '{hive_col_info['original_name']}'")
            else:
                # Pas de match direct, essayer des variantes
                possible_matches = self._find_fuzzy_matches(normalized_csv, hive_schema.keys())
                
                if possible_matches:
                    best_match = possible_matches[0]
                    hive_col_info = hive_schema[best_match]
                    mapping[hive_col_info['original_name']] = {
                        'term_guid': term_info['guid'],
                        'term_name': term_name,
                        'column_guid': hive_col_info['guid'],
                        'csv_column': csv_column,
                        'match_type': 'fuzzy',
                        'confidence': 0.8  # Score arbitraire
                    }
                    matched_count += 1
                    logger.info(f"‚ö†Ô∏è MATCH APPROXIMATIF: CSV '{csv_column}' ‚Üí Hive '{hive_col_info['original_name']}'")
                else:
                    unmatched_csv.append(csv_column)
                    logger.warning(f"‚ùå PAS DE MATCH: CSV '{csv_column}' (colonne Hive introuvable)")
        
        # R√©sum√©
        logger.info(f"üìä R√©sultat mapping: {matched_count}/{len(self.created_terms_cache)} colonnes match√©es")
        if unmatched_csv:
            logger.warning(f"‚ö†Ô∏è Colonnes CSV non match√©es: {', '.join(unmatched_csv)}")
        
        return mapping
    
    def _find_fuzzy_matches(self, csv_column, hive_columns):
        """
        Trouver des correspondances approximatives entre noms de colonnes.
        
        Logique simple:
        - Suppression des underscores
        - V√©rification de sous-cha√Ænes
        - Distance de Levenshtein simplifi√©e
        """
        matches = []
        
        csv_clean = csv_column.replace('_', '').replace('-', '')
        
        for hive_col in hive_columns:
            hive_clean = hive_col.replace('_', '').replace('-', '')
            
            # V√©rifier si l'un est contenu dans l'autre
            if csv_clean in hive_clean or hive_clean in csv_clean:
                matches.append(hive_col)
                continue
            
            # V√©rifier similarit√© (tr√®s basique)
            if self._string_similarity(csv_clean, hive_clean) > 0.7:
                matches.append(hive_col)
        
        return matches
    
    def _string_similarity(self, s1, s2):
        """Calcul basique de similarit√© entre deux cha√Ænes"""
        if not s1 or not s2:
            return 0.0
        
        # Utiliser la longueur de la sous-s√©quence commune
        longer = s1 if len(s1) >= len(s2) else s2
        shorter = s2 if len(s1) >= len(s2) else s1
        
        if len(longer) == 0:
            return 1.0
        
        # Compter les caract√®res communs
        common = sum(1 for a, b in zip(shorter, longer) if a == b)
        return common / len(longer)

    # ==================== PARTIE 4: ASSIGNATION TERMES ‚Üí COLONNES ====================
    # Cette partie fait le lien final entre glossaire business et assets techniques
    
    def assign_terms_to_hive_columns(self, table_name):
        """
        √âTAPE 6: Assigner les termes du glossaire aux colonnes Hive.
        
        CORRECTION IMPORTANTE:
        - On utilise le mapping cr√©√© pr√©c√©demment (qui garantit la coh√©rence)
        - On v√©rifie que les termes existent avant assignation
        - On g√®re les erreurs proprement
        
        C'est l'√©quivalent de "Apply terms to assets" dans Purview.
        """
        logger.info(f"üîó √âTAPE 6: Assignation des termes aux colonnes Hive")
        
        # 1. Cr√©er le mapping si pas d√©j√† fait
        mapping = self.create_csv_to_hive_mapping(table_name)
        
        if not mapping:
            logger.error("‚ùå Aucun mapping disponible pour assignation")
            return {"success": False, "error": "Pas de mapping"}
        
        # 2. Assigner chaque terme √† sa colonne
        assigned_count = 0
        failed_assignments = []
        
        for hive_column_name, mapping_info in mapping.items():
            term_guid = mapping_info['term_guid']
            column_guid = mapping_info['column_guid']
            term_name = mapping_info['term_name']
            
            logger.info(f"üîÑ Assignation: '{term_name}' ‚Üí colonne '{hive_column_name}'")
            
            # Appel API Atlas pour assigner le terme
            success = self._assign_term_to_column_api(column_guid, term_guid)
            
            if success:
                assigned_count += 1
                logger.info(f"   ‚úÖ Assign√© avec succ√®s")
            else:
                failed_assignments.append({
                    'column': hive_column_name,
                    'term': term_name
                })
                logger.error(f"   ‚ùå √âchec assignation")
        
        # 3. R√©sum√©
        result = {
            "success": assigned_count > 0,
            "total_mappings": len(mapping),
            "successful_assignments": assigned_count,
            "failed_assignments": len(failed_assignments),
            "failed_details": failed_assignments
        }
        
        logger.info(f"üìä Assignations: {assigned_count}/{len(mapping)} r√©ussies")
        
        return result
    
    def _assign_term_to_column_api(self, column_guid, term_guid):
        """
        Appel API bas niveau pour assigner un terme √† une colonne.
        
        Utilise l'endpoint /meanings d'Atlas.
        """
        try:
            assign_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{column_guid}/meanings"
            
            payload = [{
                "termGuid": term_guid,
                "relationGuid": None
            }]
            
            response = requests.post(
                assign_url,
                auth=self.auth_tuple,
                headers={'Content-Type': 'application/json'},
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return True
            else:
                logger.error(f"   API Error: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"   Exception: {e}")
            return False

    # ==================== PARTIE 5: UTILITAIRES ET CLASSIFICATIONS ====================
    
    def create_sensitivity_classifications(self):  
        """Cr√©er les classifications de sensibilit√©"""  
        enriched_metadata = self.metadata_db['enriched_metadata']  
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')  
        sensitivity_levels = [level for level in sensitivity_levels if level]  
          
        logger.info(f"Niveaux de sensibilit√©: {sensitivity_levels}")  
          
        classification_defs = []  
          
        for level in sensitivity_levels:  
            classification_def = {  
                "name": f"DataSensitivity_{level}",  
                "description": f"Classification de sensibilit√©: {level}",  
                "attributeDefs": [  
                    {"name": "sensitivity_level", "typeName": "string", "isOptional": False},  
                    {"name": "rgpd_compliant", "typeName": "boolean", "isOptional": True},  
                    {"name": "data_steward", "typeName": "string", "isOptional": True}  
                ]  
            }  
            classification_defs.append(classification_def)  
          
        if classification_defs:  
            classification_batch = {"classificationDefs": classification_defs}  
              
            response = requests.post(  
                f"{self.atlas_url}/api/atlas/v2/types/typedefs",  
                json=classification_batch,  
                auth=self.auth_tuple  
            )  
              
            if response.status_code in [200, 409]:  # 409 = d√©j√† existe
                logger.info(f"‚úÖ Classifications cr√©√©es/existantes")  
                return True  
            else:  
                logger.error(f"‚ùå Erreur classifications: {response.text}")  
                return False  
          
        return True  
    
    def wait_for_atlas_indexing(self, seconds=30):
        """
        Attendre que Atlas indexe les nouveaux √©l√©ments.
        
        IMPORTANT: Atlas n'indexe pas imm√©diatement les nouveaux termes.
        Il faut attendre que l'index Solr soit mis √† jour (g√©n√©ralement 10-30 secondes).
        
        C'est une limitation connue d'Atlas, Purview a le m√™me comportement.
        """
        logger.info(f"‚è≥ Attente indexation Atlas ({seconds}s)...")
        time.sleep(seconds)
        logger.info("‚úÖ Indexation suppos√©e termin√©e")
  
    def preview_sync_data(self):  
        """Pr√©visualiser ce qui sera synchronis√©"""  
        enriched_metadata = self.metadata_db['enriched_metadata']  
          
        total_metadata = enriched_metadata.count_documents({})  
        validated_metadata = enriched_metadata.count_documents({"validation_status": "validated"})  
        pending_metadata = enriched_metadata.count_documents({"validation_status": "pending"})  
          
        categories = enriched_metadata.distinct('recommended_rgpd_category')  
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')  
          
        preview = {  
            "total_metadata": total_metadata,  
            "validated_metadata": validated_metadata,  
            "pending_metadata": pending_metadata,  
            "rgpd_categories": [cat for cat in categories if cat],  
            "sensitivity_levels": [level for level in sensitivity_levels if level],  
            "will_sync": validated_metadata > 0  
        }  
          
        logger.info("üìä ===== PR√âVISUALISATION =====")  
        logger.info(f"Total m√©tadonn√©es: {total_metadata}")  
        logger.info(f"Valid√©es (√† sync): {validated_metadata}")  
        logger.info(f"En attente: {pending_metadata}")  
        logger.info(f"Cat√©gories RGPD: {preview['rgpd_categories']}")  
        logger.info(f"Sensibilit√©s: {preview['sensitivity_levels']}")  
          
        return preview  
    
    def create_hive_column_map(self, table_name):
     """
     Cr√©er un mapping {csv_column_name: hive_column_guid}
     AVANT de cr√©er les termes
     """
     hive_schema = self.discover_hive_schema(table_name)
    
     enriched_metadata = self.metadata_db['enriched_metadata']
     validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))
    
     mapping = {}
    
     for metadata in validated_metadata:
        csv_column = metadata['column_name'].lower()
        
        # Chercher match exact
        if csv_column in hive_schema:
            mapping[csv_column] = hive_schema[csv_column]['guid']
            logger.info(f"‚úÖ Mapping: CSV '{csv_column}' ‚Üí Hive GUID {hive_schema[csv_column]['guid'][:8]}...")
        else:
            # Fuzzy match
            matches = self._find_fuzzy_matches(csv_column, hive_schema.keys())
            if matches:
                best_match = matches[0]
                mapping[csv_column] = hive_schema[best_match]['guid']
                logger.info(f"‚ö†Ô∏è Mapping approximatif: CSV '{csv_column}' ‚Üí Hive '{best_match}'")
            else:
                logger.warning(f"‚ùå Pas de match Hive pour CSV '{csv_column}'")
    
     return mapping
    
    def _generate_business_description(self, metadata):  
        """G√©n√©rer description m√©tier riche"""  
        column_name = metadata['column_name']  
        entity_types = metadata.get('entity_types', [])  
        sensitivity = metadata.get('recommended_sensitivity_level', 'INTERNAL')  
        rgpd_category = metadata.get('recommended_rgpd_category', 'Non classifi√©')  
        total_entities = metadata.get('total_entities', 0)  
          
        description = f"""ATTRIBUT M√âTIER: {column_name.upper()}

üîç ANALYSE:
‚Ä¢ Entit√©s: {', '.join(entity_types) if entity_types else 'Aucune'}
‚Ä¢ Nombre d'entit√©s: {total_entities}
‚Ä¢ Sensibilit√©: {sensitivity}

üìã RGPD:
‚Ä¢ Cat√©gorie: {rgpd_category}

‚úÖ VALIDATION:
‚Ä¢ Valid√©: {datetime.now().strftime('%Y-%m-%d')}"""
          
        return description

    # ==================== WORKFLOW PRINCIPAL ====================
    
    
    def sync_governance_metadata(self, table_name="entites_marocaines", preview_only=False):
     """Workflow corrig√© avec assignation en 2 phases"""
    
     try:
        preview = self.preview_sync_data()
        
        if preview_only or not preview["will_sync"]:
            return {"success": False, "preview": preview}
        
        if not self._confirm_sync(preview):
            return {"success": False, "error": "Annul√©e"}
        
        # PHASE 1-5: Comme avant
        hive_schema = self.discover_hive_schema(table_name)
        if not hive_schema:
            return {"success": False, "error": "Sch√©ma Hive introuvable"}
        
        hive_columns_map = self.create_hive_column_map(table_name)
        logger.info(f"üìä {len(hive_columns_map)}/{len(hive_schema)} colonnes mapp√©es")
        
        if not self.create_sensitivity_classifications():
            return {"success": False, "error": "√âchec classifications"}
        
        glossary_guid = self.create_business_glossary()
        if not glossary_guid:
            return {"success": False, "error": "√âchec glossaire"}
        
        category_guids = self.create_rgpd_categories(glossary_guid)
        
        # PHASE 6: Cr√©er termes ET r√©cup√©rer liste pour assignation
        logger.info("\nüìù PHASE 6: CR√âATION DES TERMES")
        synced_terms, terms_to_assign = self.create_validated_metadata_terms(
            glossary_guid, 
            category_guids,
            hive_columns_map
        )
        
        logger.info(f"‚úÖ {synced_terms} termes cr√©√©s")
        
        # PHASE 7: ATTENDRE indexation (CRITIQUE!)
        logger.info("\n‚è≥ PHASE 7: ATTENTE INDEXATION")
        time.sleep(15)  # 15 secondes minimum
        
        # PHASE 8: ASSIGNER termes aux colonnes Hive
        logger.info("\nüîó PHASE 8: ASSIGNATION TERMES ‚Üí COLONNES HIVE")
        assigned_count = 0
        failed_assignments = []
        
        for assignment in terms_to_assign:
            term_guid = assignment['term_guid']
            column_guid = assignment['column_guid']
            term_name = assignment['term_name']
            column_name = assignment['column_name']
            
            logger.info(f"üîÑ Assignation: '{term_name}' ‚Üí '{column_name}'")
            
            # Utiliser l'endpoint /meanings
            assign_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{column_guid}/meanings"
            
            payload = [{
                "termGuid": term_guid,
                "relationGuid": None
            }]
            
            try:
                response = requests.post(
                    assign_url,
                    auth=self.auth_tuple,
                    headers={'Content-Type': 'application/json'},
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    assigned_count += 1
                    logger.info(f"   ‚úÖ Assign√© avec succ√®s")
                else:
                    failed_assignments.append({
                        'column': column_name,
                        'term': term_name,
                        'error': response.text
                    })
                    logger.error(f"   ‚ùå Erreur: {response.status_code} - {response.text}")
                    
            except Exception as e:
                failed_assignments.append({
                    'column': column_name,
                    'term': term_name,
                    'error': str(e)
                })
                logger.error(f"   ‚ùå Exception: {e}")
        
        self._mark_as_synced(synced_terms)
        
        return {
            "success": True,
            "glossary_guid": glossary_guid,
            "terms_created": synced_terms,
            "hive_assignments_successful": assigned_count,
            "hive_assignments_failed": len(failed_assignments),
            "failed_details": failed_assignments,
            "sync_timestamp": datetime.now().isoformat()
        }
        
     except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e)}
    
    def _confirm_sync(self, preview):
        """Demander confirmation avant synchronisation"""
        print("\n" + "=" * 80)
        print("‚ö†Ô∏è  CONFIRMATION REQUISE")
        print("=" * 80)
        print(f"M√©tadonn√©es √† synchroniser: {preview['validated_metadata']}")
        print(f"Cat√©gories RGPD: {', '.join(preview['rgpd_categories'])}")
        print(f"Niveaux de sensibilit√©: {', '.join(preview['sensitivity_levels'])}")
        print("\n‚ö†Ô∏è  Cette op√©ration va modifier Apache Atlas")
        
        response = input("\nContinuer? (oui/non): ").lower().strip()
        return response in ['oui', 'o', 'yes', 'y']
    
    def _mark_as_synced(self, synced_count):
        """Marquer les m√©tadonn√©es comme synchronis√©es dans MongoDB"""
        if synced_count > 0:
            enriched_metadata = self.metadata_db['enriched_metadata']
            enriched_metadata.update_many(
                {"validation_status": "validated"},
                {
                    "$set": {
                        "atlas_sync_status": "synced",
                        "atlas_sync_date": datetime.now()
                    }
                }
            )
            logger.info(f"‚úÖ {synced_count} m√©tadonn√©es marqu√©es comme synchronis√©es")

    # ==================== FONCTIONS DE DEBUG ====================
    
    def debug_full_workflow(self, table_name="entites_marocaines"):
        """
        Mode debug complet pour diagnostiquer les probl√®mes.
        
        Affiche:
        - √âtat MongoDB
        - √âtat Atlas (glossaire, cat√©gories, termes)
        - Sch√©ma Hive
        - Mapping potentiel
        """
        logger.info("üîß MODE DEBUG COMPLET")
        logger.info("=" * 80)
        
        # 1. √âtat MongoDB
        logger.info("\nüìä 1. √âTAT MONGODB")
        preview = self.preview_sync_data()
        
        # 2. Sch√©ma Hive
        logger.info("\nüìã 2. SCH√âMA HIVE")
        hive_schema = self.discover_hive_schema(table_name)
        
        # 3. Termes cr√©√©s
        logger.info("\nüìù 3. TERMES DANS CACHE")
        logger.info(f"Nombre de termes en cache: {len(self.created_terms_cache)}")
        for term_name, info in list(self.created_terms_cache.items())[:5]:
            logger.info(f"  - {term_name} ‚Üí {info['source_column']}")
        
        # 4. Simulation mapping
        logger.info("\nüîó 4. SIMULATION MAPPING")
        if self.created_terms_cache and hive_schema:
            mapping = self.create_csv_to_hive_mapping(table_name)
            logger.info(f"Mappings possibles: {len(mapping)}")
        
        return {
            "mongodb_state": preview,
            "hive_columns": len(hive_schema),
            "cached_terms": len(self.created_terms_cache),
            "potential_mappings": len(mapping) if 'mapping' in locals() else 0
        }

def main():
    """
    Point d'entr√©e principal avec options.
    
    UTILISATION:
    1. Mode normal: Synchronisation compl√®te
    2. Mode preview: Voir ce qui sera fait sans modifier Atlas
    3. Mode debug: Diagnostiquer les probl√®mes
    """
    print("=" * 80)
    print("ATLAS METADATA GOVERNANCE - SYNCHRONISATION RGPD")
    print("=" * 80)
    print("\nModes disponibles:")
    print("1. Preview (voir sans modifier)")
    print("2. Sync complet (avec confirmation)")
    print("3. Debug (diagnostiquer les probl√®mes)")
    
    choice = input("\nVotre choix (1/2/3): ").strip()
    
    governance = AtlasMetadataGovernance()
    
    if choice == "1":
        # MODE PREVIEW
        result = governance.sync_governance_metadata(preview_only=True)
        print("\n" + "=" * 80)
        print("PR√âVISUALISATION")
        print("=" * 80)
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
    elif choice == "2":
        # MODE SYNC COMPLET
        result = governance.sync_governance_metadata(preview_only=False)
        print("\n" + "=" * 80)
        print("R√âSULTAT SYNCHRONISATION")
        print("=" * 80)
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
        if result.get("success"):
            print("\n‚úÖ SUCC√àS!")
            print(f"Termes cr√©√©s: {result.get('terms_created', 0)}")
            print(f"Assignations: {result.get('assignment_result', {}).get('successful_assignments', 0)}")
            
            # Diagnostics si assignations √©chou√©es
            failed = result.get('assignment_result', {}).get('failed_assignments', 0)
            if failed > 0:
                print(f"\n‚ö†Ô∏è {failed} assignations √©chou√©es")
                print("Causes possibles:")
                print("- Noms de colonnes diff√©rents entre CSV et Hive")
                print("- Termes pas encore index√©s (attendre 1-2 minutes)")
                print("- V√©rifiez les logs d√©taill√©s ci-dessus")
        else:
            print(f"\n‚ùå √âCHEC: {result.get('error')}")
            
    elif choice == "3":
        # MODE DEBUG
        result = governance.debug_full_workflow()
        print("\n" + "=" * 80)
        print("RAPPORT DEBUG")
        print("=" * 80)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    else:
        print("Choix invalide")

if __name__ == "__main__":
    main()